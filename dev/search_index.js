var documenterSearchIndex = {"docs":
[{"location":"RandomVariables/#AbstractRandomVariables-interface","page":"AbstractRandomVariables interface","title":"AbstractRandomVariables interface","text":"","category":"section"},{"location":"RandomVariables/","page":"AbstractRandomVariables interface","title":"AbstractRandomVariables interface","text":"The abstract random variables interface provides a method for calcaulting the expected value of random varaibles in the Bellman equations. The idea behind this interface is that the values the random variable take on is given in a matrix with d by N entries called nodes where d is the dimension of the random variable and N is the number of points to evaluate. The probability mass at each point is given in a vector called weights. The expected value of a function f of a random variable X can then be calcualted by evaluating the function at each of the columsn in nodes and multiplying these values by the weights.","category":"page"},{"location":"RandomVariables/","page":"AbstractRandomVariables interface","title":"AbstractRandomVariables interface","text":"EfX = sum(X.weights.*mapslices(f,X,dims=1))","category":"page"},{"location":"RandomVariables/","page":"AbstractRandomVariables interface","title":"AbstractRandomVariables interface","text":"If X is a discrete random variable then X.weights is the probability mass function. This framework can also be used for continuous random variables by replacing the nodes and weights with the nodes and weights of a quadrature scheme like guass-hermite weights and nodes for a normal distribution. The frameowrk can also be used for montecarlo integration by sampling the nodes at random and setting the weights equal to 1/N.","category":"page"},{"location":"RandomVariables/","page":"AbstractRandomVariables interface","title":"AbstractRandomVariables interface","text":"The AbstractRandomVariables interface provides methods to construce normally distributed random variables using gauss-hermite qudrature (GaussHermiteRandomVariable), set up monte carlo integration using MCRandomVariable, build markov cains with MarkovChain and sample_markov_chain, and buidl custome discrete random variables and quadratures using RandomVariable. Details on each method are provided in the API. ","category":"page"},{"location":"#ValueFunctionIterations.jl","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"","category":"section"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"ValueFunctionIterations.jl is a package for solving Markov decision processes (MDP) using the value function iteration algorithm. The library is designed to solve MDPs with continuous state variables, although, it can also accomidate discrete variables. ","category":"page"},{"location":"#Markov-decision-processes","page":"ValueFunctionIterations.jl","title":"Markov decision processes","text":"","category":"section"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"Markov decision processes are a type of optimization problem where a sequence of actions u_t are chosen to maximize a sequence of rewards R discounted into the future by a factor delta. The rewards R(s_tu_tX_t) depend on the action taken u_t, random events X_t and thirds set of variables s_t called state variables","category":"page"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"V = undersettextmaxu_tleftsum_t=0^inftydelta R(u_ts_tX_t) right","category":"page"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"The state variables are influnced by the actions u_t creating cause and effect relationships between actions taken in the present and future rewards. These relationships are captured by a function called the state transition function F(s_tu_tX_t) which determines the value of the state variables in the next period s_t+1.","category":"page"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"Markov decision provesses find the optimum sequence of action u_t to maximize the discounted sum of rewards given the efect of each action on rewards in the present and on rewards in the future. Markov decision process also allow for there to be uncertianty in the relationship between current actions and future rewards captured by the random variable X_t. ","category":"page"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"Markov decision process can be solved using the Bellman equation which provides recursively calculates the expected value of the objective V as a function of the current value fo the state variables s_t","category":"page"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"V(s_t) = undersettextmaxu_tleftE_XR(s_tu_tX_t)+delta E_XV(s_tu_tX_t) right","category":"page"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"The Bellman equaiton can be solved by approximating the value function V and the expectation operatoes E_x numerically. This software package uses BSplines to approximate the value fucntion and allows useres to choose between Montecarlo integration and quadrature to solve the expectations.  ","category":"page"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"Markov decision process are common in natural resource and financial economics where individuals make decision about how operate in uncertain environments where actions in the present influence future possibilities. For example, a retired persion might have to choose how to spend thier savings over time given uncertain returns on their investments and health expenses.  ","category":"page"},{"location":"#How-to","page":"ValueFunctionIterations.jl","title":"How to","text":"","category":"section"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"MDPs can be defined and solved using the DynamicProgram function. This function takesa function that defines the rewards R(s,u,X,p), the state transition function F(s,u,X,p), a component vector of parameters p defined usng the ComponentArrays library, a matrix u with all possible actions, a random variable object X, the discount factor delta, and a regular set of grid point for each dimension of the state space grid.... The grids must be define as an AbstractRange (e.g., 0:0.1:1) and the random variables X are defined as and AbstractRandomVariable. Constructors for AbstractRandomVariables are included in the ValueFunctionIterations.jl package. ","category":"page"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"The following example defines a dyamic program to estimate the optimal rotation time for a timber stand that has some probability of experince damage before harvest. ","category":"page"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"using Plots, ValueFunctionIterations, ComponentArrays, Distribtions\n\n# Income from harvesting trees\nfunction R(s,u,X,p)\n    if X[1] == 0.0 && u[1] == 0 # if neither damage or harvest recieve nothing\n        return 0\n    elseif u[1] == 1 # If harvesting occurs recive net revenue \n        return s[1]-p.c\n    else X[1] == 1 # If damage occurs, harvest and revice the salvage value (X[2])\n        return X[2]*s[1]-p.c\n    end\nend\n\n# State update function \nfunction F(s,u,X,p)\n    if X[1] == 0.0 && u[1] == 0 # if neither damage or harvest allow growth\n        return s[1]*exp(p.r*(1-s[1]/p.k))\n    else  # If harvesting or damage occurs go to replanted biomass \n        return 0.02\n    end\nend \n\n# Parameters \n# r: growth rate, k: maximum growth, c: cost of harvest, p_s: price for damaged timber \np = ComponentArray(r = 0.15, k = 1.0, c = 0.25)\n\n# Harvest levels (0 or 97.5%)\nu = action_space([0.0,1.0])\n\n# Damage levels (0 or 97.5%) and probabilities (0.99 and 0.01)\nX1 = RandomVariable([0.0 1.0;], [0.99, 0.01])\n\n# Define quadrature scheme for normally distributed salvage values using Gauss-Hermite quadrature \nX2 = GaussHermiteRandomVariable(10,[0.5],[0.1^2;;])\n\n# Combine the two random variables \nX = product(X1,X2)\n\n\n# Discount factor \nδ = 0.99\n\n# Grid of stand sizes\ngrid = 0.01:0.01:1.00\n\n# Define and solve the dynamic program\nsol = DynamicProgram(R, F, p, u,  X, δ, grid; tolerance = 1e-5)\n\n# Plot the policy function \nPlots.plot(grid,broadcast(s -> s - s*sol.P(s)[1],grid), xlabel = \"Standing timber\",\n             ylabel = \"Board feet\", label = \"Standing timber\",linewidth = 2)\nPlots.plot!(grid,broadcast(s -> s*sol.P(s)[1],grid), label = \"Harvest\", linewidth = 2)\nplot!(size = (400,250))","category":"page"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"(Image: )","category":"page"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"Once we have a solved problem we can run simulations under the optimal policy using the simulate function.","category":"page"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"states,actions,rewards,vals=simulate(sol,100)\nPlots.plot(states[1,:], label = \"Standing timber\", linewidth = 2)\nPlots.plot!(actions[1,:].*states[1,1:(end-1)], label = \"Harvest\", linewidth = 2)\nPlots.plot!(vals[1:(end-1)], label = \"Present value\", linewidth = 2)\nplot!(size = (400,250), xlabel = \"Time\", ylabel = \"Value\")","category":"page"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"(Image: ) Dynamic programs can take along time to run so you can build a problem without running it and evaluate the computation time by setting the solve key word argument in the  DynamicProgram function to false. You can then run estimate_time on the model to evaluate performance bottle necks and then use solve! to run the model once you are happy with the predicted run time. ","category":"page"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"prob = DynamicProgram(R, F, p, u,  X, δ, grid; solve = false, tolerance = 1e-5)\nestimate_time(prob)\nsolve!(prob)","category":"page"},{"location":"#Package-contents","page":"ValueFunctionIterations.jl","title":"Package contents","text":"","category":"section"},{"location":"","page":"ValueFunctionIterations.jl","title":"ValueFunctionIterations.jl","text":"Pages = [\"API.md\", \"RandomVariables.md\"]","category":"page"},{"location":"API/#API","page":"API","title":"API","text":"","category":"section"},{"location":"API/#ValueFunctionIterations.DynamicProgram","page":"API","title":"ValueFunctionIterations.DynamicProgram","text":"DynamicProgram\n\nStores the data required to define a dynamic programming problem along with the value V and policy functions P. ...\n\nElements:\n\n- R: the reward function, takes the form R(s,u,X,p) where s is the state and u is the decision variable, X is random varible inputs and p are paramters\n- F: the state update function, takes the form F(s,u,X,p).\n- p: the state update parameters, ComponentArray it must be compatable with both R and F. \n- u: the decision variables, a matrix where each column give a possibel valueof the decision variable u. \n- X: the random variables, an AbstractRandomVariable object.\n- δ: the discount factor, Float64.\n- V: the value function, a AbstractValueFunction object.\n- P: the policy function, a AbstractValueFunction object.\n\n\n\n\n\n","category":"type"},{"location":"API/#ValueFunctionIterations.DynamicProgram-Tuple{Function, Function, ComponentArrays.ComponentArray, Matrix{Float64}, ValueFunctionIterations.AbstractRandomVariable, Float64, Vararg{Any}}","page":"API","title":"ValueFunctionIterations.DynamicProgram","text":"DynamicProgram(R::Function, F::Function,  p::ComponentArray, u::Matrix{Float64}, X::AbstractRandomVariable, δ::Float64, grid...; kwrds...  )\n\nSolves a continuous state, discrete action dynamic optimization problem using value function iteration and returns  the solution as a DynamicProgram object.  ...\n\nArguments:\n\n- R: the reward function, takes the form R(s,u,X,p) where s is the state and u is the decision variable, X is random varible inputs and p are paramters\n- F: the state update function, takes the form F(s,u,X,p). \n- p: the state update parameters, ComponentArray it must be compatable with both R and F. \n- u: the decision variables, a matrix where each column give a possibel valueof the decision variable u. \n- X: the random variables, an AbstractRandomVariable object.\n- δ: the discount factor, Float64. \n- grid...: evenly spaced grid points for each dimension of the state space, variable number of arguents are allowed.\n\nKeyword arguments:\n\n- solve: whether to solve the problem, defaults to \"conditional\". The problem will not solve if the estiamted time is larger than ten minutes, this can be over ridden by setting solve = true to always solve or solvefalse to never solve. `.\n- v0: the initial value of the value function, defaults to 0.0\n- order_value: the order of the interpolation for the value function, defaults to Cubic(Line(OnGrid()))\n- order_policy: the order of the interpolation for the policyfunction, defaults to Constant()\n- extrap: the value to use for extrapolation, defaults to Float()\n- tolerance: the tolerance for VFI convergence, defaults to 1e-5\n- maxiter: the maximum number of iterations, defaults to round(Int, 3/(1-δ))\n\nValues:\n\n- a DynamicProgram object\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.MCRandomVariable","page":"API","title":"ValueFunctionIterations.MCRandomVariable","text":"MCRandomVariable\n\nA struct that represents a mulitvariate random variable with samples that can be updated in place. ... Elements:     - N: the number of samples     - dims: the dimension of the random variable     - nodes: a matrix of samples     - weights: a vector of weights (1/N)     - sample: a function for drawing samples from the distribution\n\n\n\n\n\n","category":"type"},{"location":"API/#ValueFunctionIterations.MCRandomVariable-Tuple{Function, Int64}","page":"API","title":"ValueFunctionIterations.MCRandomVariable","text":"MCRandomVariable(sample::Function, N::Int)\n\nInitializes an instance of a MCRandomVariable using a sampler function and  a desired number of samples. \n\nThe MCRandomVariable stores the sampels in a matrix of size dims x N where dims is the  dimension of the random variable and N is the number of samples. Calling the MCRandomVariable object as a function will update the samples in place allowing for memory efficent resampling. \n\nThe MCRandomVariable object also stores a vector of weights which are initialized to 1/N. This allows the MCRandomVariable to be substituted for quadrature schemes represetned by  the RandomVaraibles.jl interface.  ...\n\nArguments:\n\n- sample: a function for drawing samples from the distribution\n- N: the number of samples\n\nValues:\n\n- a MCRandomVariable object\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.MCRandomVariable-Tuple{Function, RandomVariable, Int64}","page":"API","title":"ValueFunctionIterations.MCRandomVariable","text":"MCRandomVariable(sample::Function, N::Int)\n\nInitializes an instance of a  MCRandomVariable using a sampler function and  a desired number of samples and a RandomVariable object.\n\nThis function will initialize a MCRandomVariable object that represents the cartesian product of the variable represented by the sample funciton and X. Thsi is useful if  you wants to represent the product of a continuous random variable with montecarlo methods  and a discrete random variable by taking weighted sums. \n\nThe MCRandomVariable object stores the samples in a matrix of size d x (M x N)  where d is the  dimension of the random variable, and m is the number of nodes in X. Thw first elements of each node  represent the sample from the sample function. The remaining elements represent the nodes from X.\n\nThe weights for each node are equal to 1/N times the corresponding weight from X. ...\n\nArguments:\n\n- sample: a function for drawing samples from the distribution\n- X: a RandomVariable object \n- N: the number of samples\n\nValues:\n\n- a MCRandomVariable object\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.RandomVariable","page":"API","title":"ValueFunctionIterations.RandomVariable","text":"RandomVariable\n\nThis class defines a quadrature scheme for multi variate random varaibles. The nodes are a matrix of values with each colum corresponding  to a point in the set of possible samples and the weights give the probabiltiy of that point.  ... Elements:     - nodes: a matrix of values     - weights: a vector of weights\n\n\n\n\n\n","category":"type"},{"location":"API/#ValueFunctionIterations.GaussHermiteRandomVariable-Tuple{Int64, AbstractVector{Float64}, AbstractMatrix{Float64}}","page":"API","title":"ValueFunctionIterations.GaussHermiteRandomVariable","text":"GaussHermiteRandomVariable(m::Int64,mu::AbstractVector{Float64},Cov::AbstractMatrix{Float64})\n\nReturns an RandomVariable with weights and nodes for a multivariate normal distribution with covariance matrix Cov and mean vector mu. The weights and nodes are chosen using a guass hermite  quadrature scheme. \n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.MarkovChain-Tuple{Any}","page":"API","title":"ValueFunctionIterations.MarkovChain","text":"MarkovChain(p)\n\nBuilds RandomVariable object that represents a markov chain and is compatable with ValueFunctionIterations.jl. The goal of this object is to  take expectations over the outcome of a markov chain in the  most efficnet possible way using the RandomVariables.jl interface. \n\nThe RandomVariable object is definged to work with the sample_markov_chain function.  The nodes are intended to be passed to the sample_markov_chain function as the random number argumet. If the nodes are sampled using the weights stored in the random varible object and passed to  sample_markov_chain the results wil be the same as if a uniform random number was sampled. \n\nThis allows the weights and nodes to be used to calcualte expectations over the outcome of the markov chain. with the minimum number of computations possible without changing the weights as a function of the current state.  ...\n\nArguments:\n\n- p: a transition probability matrix of size m\n\nValues:\n\n- a RandomVariable object\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.action_space-Tuple","page":"API","title":"ValueFunctionIterations.action_space","text":"action_space_product(U...)\n\nReturns the cartesian product of the action spaces U defined at abstract rance objects. ...\n\nArguments\n\n- U: two or more AbstractRange objects\n\nValues\n\n- a matrix with the cartesian product of the action spaces U.\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.estimate_time-Tuple{DynamicProgram{Matrix{Float64}}}","page":"API","title":"ValueFunctionIterations.estimate_time","text":"estimate_time(DP::DynamicProgram)\n\nEstimate how long a dynamic program will take to solve. ...\n\nArguments:\n\n- DP: a DynamicProgram object\n\nValue\n\n- a dictionary with keys \"Estimate\", \"One call\", \"Number of computations\", \"Estimated iterations\"\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.get_actions_function-Tuple{DynamicProgram}","page":"API","title":"ValueFunctionIterations.get_actions_function","text":"get_value_function(DP::DynamicProgram)\n\nReturns the action space of a dynamic program DP. ...\n\nArguments:\n\n- DP: a DynamicProgram object\n\nValues:\n\n- a matrix of size dims(u) by N actions\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.get_discount_factor_function-Tuple{DynamicProgram}","page":"API","title":"ValueFunctionIterations.get_discount_factor_function","text":"get_value_function(DP::DynamicProgram)\n\nReturns the discount factor for a the dynamic program DP. ...\n\nArguments:\n\n- DP: a DynamicProgram object\n\nValues:\n\n- a Float object\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.get_parameters_function-Tuple{DynamicProgram}","page":"API","title":"ValueFunctionIterations.get_parameters_function","text":"get_value_function(DP::DynamicProgram)\n\nReturns the paramters of the dynamic program DP. ...\n\nArguments:\n\n- DP: a DynamicProgram object\n\nValues:\n\n- a ComponentVector of model parameters\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.get_policy_function-Tuple{DynamicProgram}","page":"API","title":"ValueFunctionIterations.get_policy_function","text":"get_value_function(DP::DynamicProgram)\n\nReturns the policy function of the dynamic program DP. ...\n\nArguments:\n\n- DP: a DynamicProgram object\n\nValues:\n\n- an AbstractValueFunction object\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.get_random_variables_function-Tuple{DynamicProgram}","page":"API","title":"ValueFunctionIterations.get_random_variables_function","text":"get_value_function(DP::DynamicProgram)\n\nReturns the random variable for a the dynamic program DP. ...\n\nArguments:\n\n- DP: a DynamicProgram object\n\nValues:\n\n- an AbstractRandomVariable object\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.get_reward_function-Tuple{DynamicProgram}","page":"API","title":"ValueFunctionIterations.get_reward_function","text":"get_value_function(DP::DynamicProgram)\n\nReturns the reward function of the dynamic program DP.\n\nArguments:     - DP: a DynamicProgram object\n\nValues:     - a function with arguments (s,u,X,p)\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.get_update_function-Tuple{DynamicProgram}","page":"API","title":"ValueFunctionIterations.get_update_function","text":"get_value_function(DP::DynamicProgram)\n\nReturns the state update function of the dynamic program DP. ...\n\nArguments:\n\n- DP: a DynamicProgram object\n\nValues:\n\n- a function with arguments (s,u,X,p)\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.get_value_function-Tuple{DynamicProgram}","page":"API","title":"ValueFunctionIterations.get_value_function","text":"get_value_function(DP::DynamicProgram)\n\nReturns the value function of the dynamic program DP. ...\n\nArguments:\n\n- DP: a DynamicProgram object\n\nValues:\n\n- an AbstractValueFunction object\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.product-Tuple{RandomVariable, RandomVariable}","page":"API","title":"ValueFunctionIterations.product","text":"product(X:: RandomVariable, Y:: RandomVariable)\n\nReturns a RandomVariable that is the cartesian product of two independent RandomVariables.  ...\n\nArguments\n\n- X: a RandomVariable\n- Y: a RandomVariable\n\nValues\n\n- a RandomVariable\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.sample_discrete-Tuple{Any, Any}","page":"API","title":"ValueFunctionIterations.sample_discrete","text":"sample_discrete(p,rng)\n\nSamples an index 1:n with probability mass for each index given by p,  given a draw from a uniform random variable rng on the set (0,1). ...\n\nArguments:\n\n- p: a vector of probabilities\n- rng: a number in [0,1]\n\nValues:\n\n- an integer in 1:length(p)\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.sample_markov_chain-Tuple{Any, Any, Any}","page":"API","title":"ValueFunctionIterations.sample_markov_chain","text":"sample_markov_chain(x,p,rng)\n\nSamples from a markov chain given the current state x and the transition matrix p using a uniform random variable rng on the unit interval. ...\n\nArguments:\n\n- x: the current state (integer in 1:m)\n- p: a transition probability matrix of size m\n- rng: a number in [0,1]\n\nValues:\n\n- an integer in 1:m\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.simulate-Tuple{DynamicProgram, Int64}","page":"API","title":"ValueFunctionIterations.simulate","text":"simulate(DP::DynamicProgram,T::Int)\n\nSimulates the dynamic program DP under the optimal policy for T timesteps. The function returns the states in a matrix of size dims(s) by T+1,  action in a matrix of size dim(u) by T,reward in a vector of size T,  and value in a vector of size T+1. ...\n\nArguments:\n\n- DP: a DynamicProgram object\n- T: the number of timesteps to simulate\n\nValues:\n\n- the states in a matrix of size dims(s) by T+1\n- the actions in a matrix of size dim(u) by T  \n- the rewards in a vector of size T\n- the values in a vector of size T+1\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.simulate-Tuple{DynamicProgram, MCRandomVariable, Int64}","page":"API","title":"ValueFunctionIterations.simulate","text":"simulate(DP::DynamicProgram,X::MCRandomVariable,T::Int)\n\nSimulates the dynamic program DP under the optimal policy for T timesteps.  Sampling the random variables at each timestep using the sampler in X.  The function returns the states in a matrix of size dims(s) by T+1,  action in a matrix of size dim(u) by T,reward in a vector of size T,  and value in a vector of size T+1. ...\n\nArguments:\n\n- DP: a DynamicProgram object\n- X: a MCRandomVariable object\n- T: the number of timesteps to simulate\n\nValues:\n\n- the states in a matrix of size dims(s) by T+1\n- the actions in a matrix of size dim(u) by T  \n- the rewards in a vector of size T\n- the values in a vector of size T+1\n\n\n\n\n\n","category":"method"},{"location":"API/#ValueFunctionIterations.solve!-Tuple{DynamicProgram}","page":"API","title":"ValueFunctionIterations.solve!","text":"solve!(DP::DynamicProgram; kwrds...)\n\nRus the VFI algorithm to solve the dynamic program DP and updates the value and policy fuctions in place ...\n\nArguments:\n\n- DP: a DynamicProgram object\n\nKey words:\n\n- order_policy: the order of the interpolation for the policyfunction, defaults to Constant()\n- tolerance: the tolerance for VFI convergence, defaults to 1e-5\n- maxiter: the maximum number of iterations, defaults to round(Int, 3/(1-δ))\n\n\n\n\n\n","category":"method"}]
}
